---
toc: true
toc-depth: 4
toc-expand: false
---
## Documentation
### Write a better README
A README is a plaintext file that includes a detailed description of a project. It's typically located in the root directory, and often includes:

- A broad overview of what the project does, why it was made, and the rationale behind the way it was implemented
- Instructions for how to install or run the code
- Links to other related tools or resources
- Recognition of various contributors
- Status updates, feature roadmaps, and an overview of the project's overall orientation

[makeareadme.com](https://www.makeareadme.com/) is a great resource that provides advice for writing clear and helpful readmes, including a template that you may adapt for your own purposes.

### Add comments to old code or code in progress
Code comments appear as silent (i.e. non-functional) text embedded in the source code. They communicate what functions do by providing a brief description of how they are situated in broader workflows.
They may also include notes and task lists for iteratively improving existing functions.

Some qualities of good code comments include:

::: {.columns-bullets}
- Clear and concise
- Easy to remove once a fix has been implemented
- Explain the "why", not the "what"
- Written for your future self
:::

Here is a great write-up about how to write effective code comments: [How to Write Professional Code Comments: A Beginner's Guide to Better Code Documentation (dev.to)](https://dev.to/anurag_dev/how-to-write-professional-code-comments-a-beginners-guide-to-better-code-documentation-27hf)

Additionally, here are some great examples of code comments written by archaeologists:

::: {.panel-tabset}
## Single-line
From Joe Roe's [paleoclim](https://github.com/joeroe/rpaleoclim/blob/master/R/paleoclim.R):

```r
# Download
if (!fs::file_exists(tmpfile) | isTRUE(skip_cache)) {
  raster <- download_paleoclim(url, tmpfile, as, quiet)
}
```

## Multi-line
From Joe Roe's [paleoclim](https://github.com/joeroe/rpaleoclim/blob/master/R/paleoclim.R):

```r
#' Load data from PaleoClim
#'
#' Loads a PaleoClim data file (`.zip` format) into R as a `SpatRaster`.
#'
#' @param file Character. Path to a *.zip file downloaded from PaleoClim.
#' @param as          Character. `as = "raster"` returns a `RasterStack` object
#'                    (see [raster::stack()]) instead of the default raster from
#'                    the `terra` package. It is provided for backwards
#'                    compatibility and will be removed in future versions.
#'                    Requires the `raster` package.
#'
#' @return
#' `SpatRaster` object (see [terra::rast()]) with each bioclimatic variable
#' as a separate named layer.
#'
#' @export
#'
#' @examples
#' file <- system.file("testdata", "LH_v1_10m_cropped.zip",
#'                     package = "rpaleoclim")
#' load_paleoclim(file)
load_paleoclim <- function(file, as = c("terra", "raster")) {
  as <- rlang::arg_match(as)

  tmpdir <- fs::file_temp("paleoclim_")
  utils::unzip(file, exdir = tmpdir)

  tifs <- fs::dir_ls(tmpdir, recurse = TRUE, glob = "*.tif")
  names(tifs) <- fs::path_ext_remove(fs::path_file(tifs))

  if (length(tifs) > 0) raster <- terra::rast(tifs)
  else raster <- terra::rast()

  if (as == "raster" ) {
    if (!requireNamespace("raster", quietly = TRUE)) {
      rlang::abort(
        '`as = "raster"` requires package `raster`',
        class = "rpaleoclim_missing_package"
      )
    }
```

## Documentation
From Joe Roe's [paleoclim](https://github.com/joeroe/rpaleoclim/blob/master/R/paleoclim.R):

```r
#' Retrieve data from PaleoClim
#'
#' Downloads data from PaleoClim (<http://www.paleoclim.org>) and loads it into R
#' as a `SpatRaster` object.
#'
#' @param period      Character. Time period to retrieve.
#' @param resolution  Character. Resolution to retrieve.
#' @param region      `SpatExtent` object or object that can be coerced to
#'                    `SpatExtent` (see [terra::ext()]), describing the
#'                    region to be retrieved. If `NULL`, defaults to the whole
#'                    globe.
#' @param as          Character. `as = "raster"` returns a `RasterStack` object
#'                    (see [raster::stack()]) instead of the default raster from
#'                    the `terra` package. It is provided for backwards
#'                    compatibility and will be removed in future versions.
#'                    Requires the `raster` package.
#' @param skip_cache  Logical. If `TRUE`, cached data will be ignored.
#' @param cache_path  Logical. Path to directory where downloaded files should
#'   be saved. Defaults to R's temporary directory.
#' @param quiet       Logical. If `TRUE`, suppresses messages and download
#'   progress information.
#'
#' @details
#' See <http://www.paleoclim.org> for details of the datasets and codings.
#' Data at 30s resolution is only available for 'cur' and 'lgm'.
#'
#' By default, `paleoclim()` will read previously downloaded files in R's
#' temporary directory if available. Use `skip_cache = TRUE` to override this.
#' `cache_path` can also be set to another directory. This can be useful if you
#' want to reuse downloaded data between sessions.
#'
#' @return
#' `SpatRaster` object (see [terra::rast()]) with each bioclimatic variable
#' as a separate named layer.
#'
#' @export
#'
#' @examplesIf interactive() && curl::has_internet()
#' paleoclim("lh", "10m")
```

## Inline

## TODO

:::

### Add a citation file
A [cff file](https://citation-file-format.github.io/) is a plaintext file with .cff extension containing information that can be picked up by a reference manager to generate clean citations. It follows a yaml format, which is both human- and machine-readable.

See the example below from Maddison Simon's and Sophie Schmidt's [Percopackage](https://github.com/SCSchmidt/percopackage/) for some common information to include in your cff:

```yaml
cff-version: 1.2.0
message: "If you use this software, please cite it as below."
authors:
  - family-names: Maddison
    given-names: M. Simon
    orcid: https://orcid.org/0000-0001-5006-6604
  - family-names: Schmidt
    given-names: Sophie C.
    orcid: https://orcid.org/0000-0003-4696-2101
title: "Percopackage"
version: 0.0.0.9001
identifiers:
  - type: doi
    value: 10.17605/OSF.IO/7EXTC
date-released: 2024-08-24
```

### Writing comprehensive documentation
When a README file is not enough, or if you want to e.g. host your code documentation on a separate website, you can use a _documentation generator_ such as [Sphinx](https://www.sphinx-doc.org/en/master/index.html). Sphinx is a tool that generates a series of HTML files (or PDF) starting from a directory containing reStructuredText or Markdown files.

Sphinx can be extended in various ways and it can power a whole website including other elements such as blog pages, homepages or books. However, it is primarily used to write documentation websites. To get a sense of what this looks like, look at many big documentation websites which are powered by Sphinx, such as the [Python documentation](https://docs.python.org/3/) and the [Linux Kernel documentation](https://docs.kernel.org/). Other extensions allow e.g. to use LaTeX scripts within the documentation files.

Sphinx can be set up easily: after installation it's enough to run `sphinx-quickstart` from a project folder to create the necessary boilerplate to host the documentation. The documentation can then be extended in whatever way it is preferred. A particularly useful resource within Sphinx is autodoc, which can populate an API documentation page using docstrings from functions in the code. 

For example, we can document a function within a code as (example from [CodeRefinery lecture on documentation](https://coderefinery.github.io/documentation/sphinx/#exercise-sphinx-autodoc), [DOI](doi.org/10.5281/zenodo.8280234))

```python
def multiply(a: float, b: float) -> float:
    """
    Multiply two numbers.

    :param a: First number.
    :param b: Second number.
    :return: The product of a and b.
    """
    return a * b
```

With Sphinx autodoc set up, the multiline comment would be pulled into an "API reference" section. This highlights the importance of writing documentation at the level of the code, and Sphinx allows to access documentation in a specific, well organized location. Because the resulting files are generally HTML, the documentation can then be hosted on a server as its own website, or as part of another website, such as a Github Pages webpage (as in Matteo Tomasini's example on [MetaPypulation](https://mtomasini.github.io/MetapopulationsPython/) (see the code [here](https://github.com/mtomasini/MetapopulationsPython/tree/main/docs)).

Because the Sphinx documentation needs to be rebuilt after each change, it's a good idea to automatize the process through the usage of Github Actions or other CI tools.


## Longevity
### Archive your code in a long-term repository
While commercial platforms like GitHub and GitLab may provide very useful collaboration features, they should not be trusted as long-term stewards of software or data. Professional archives should instead be used to backup public-facing code repositories.

[Zenodo](https://zenodo.org/) is a general-purpose open repository developed under the European OpenAIRE program and operated by CERN, which allows researchers to deposit research papers, data sets, research software, reports, and any other research related digital artefacts. One of its key features is direct [integration with GitHub](https://docs.github.com/en/repositories/archiving-a-github-repository/referencing-and-citing-content), and once configured, saves a snapshot of each release. For each submission, a persistent digital object identifier (DOI) is minted.

[Software Heritage](https://www.softwareheritage.org/) is another archive dedicated to preserving publicly accessible code. It regularly crawls several software forges and package indexes, and anyone can [upload their own repositories](https://archive.softwareheritage.org/save/) too.

### Archive abandoned projects
It may be prudent to acknowledge when it's time to tuck a project away and mark it as [archived](https://docs.github.com/en/repositories/archiving-a-github-repository/archiving-repositories). This will put a banner on your repo and restrict any new pull requests.

If a project has been picked up by others, or the work continues in another forum, make sure to provide a link in the original README!

### Develop a release strategy
Sharing code as a series of stable releases makes it easier to share code with predictable functionality. As new features are implemented or when the codebase changes in substantial ways, the software should be rolled into a new release. This enhances clarity for what users can expect when they download a specific release. It also makes it easier to communicate and de-bug issues raised by users by ensuring that all parties are accessing the same codebase.

There are [numerous schemes](https://nesbitt.io/2024/06/24/from-zerover-to-semver-a-comprehensive-list-of-versioning-schemes-in-open-source.html) for releasing code, which enable developers to communicate the degree of changes made between versions. Some of these include:

- [Semantic Versioning](https://semver.org/): Major, Minor or Patch versions indicate breaking changes relative to prior versions.
- [Alternative Versioning (AltVer)](https://altver.org/): Tracks implementation of major overhauls, new features, and careful maintenance.
- [Calendar Versioning (CalVer)](https://calver.org/): For "rolling" releases that follow a regular maintenance cycle.

## Productivity
### [in progress:]{.mark} Open issues
- On other projects
- On your own projects

### Develop a project roadmap
It may be helpful to develop a plan of action to structure future work. Some factors to consider include:

- What features are on the horizon?
- What steps need to be taken to achieve them?
- What resources will be needed to ensure effective action?
- Who can I call on for support?

Effective roadmaps have tangible and measurable outcomes, and are comprised of tasks that can actually be acted upon.
See [this resource](https://open-innovation-projects.org/blog/creating-a-comprehensive-open-source-project-roadmap-a-step-by-step-guide-for-successful-development) for some great tips on how to write great roadmaps.


## Collaboration and Community-Building
### Add a license
Licenses describe legal rights for accessing, using and modifying code. Open licenses, which enable widespread and uninhibited use, are increasingly common, however copyleft or opinionated licenses impose restrictions that add conditions based on specific use-cases. For instance, [CC-NC limits reuse to non-commercial applications](https://wiki.creativecommons.org/wiki/NonCommercial_interpretation), [copyleft](https://en.wikipedia.org/wiki/Copyleft) forces downstream applications to adopt an equally permissive license for any derivative works, and numerous other licenses are aligned with specific social causes. [The Hippocratic License hosts a tool](https://firstdonoharm.dev/build/) for creating a custom license, including explicit allowances and restrictions for various potential use-cases, such as a Fossil Fuel Divestment module, a Law Enforcement module, and a Supply Chain Transparency module.

Visit [choosealicense.com](https://choosealicense.com/) for an extremely detailed survey and comparison of various software and non-software licenses. There is also a page [describing what happens if you don't choose a license](https://choosealicense.com/no-permission/).

Note that software licenses are often distinct from licenses intended for non-software materials, such as data, media, documentation and fonts.

### Acknowledge contributors
Open source projects thrive based on community support, and acknowledging contributors is a great way to maintain enthusiasm among the team. This can be done in the README, on social media, and in any other venue where you share your work (journal articles, conference presentations, etc).

When acknowledging the people behind the code, be sure to describe **_how_** people contribute, including specific things they've done to move the project forward.

Highlighting the roles of contributors coming from under-represented backgrounds may also help foster more inclusive developer communities, which is great for enhancing the quality of both communities and codebases.

### List your project in a community registry
It may be beneficial to include your project in a community registry to ensure that the work is findable and accessible by target audiences.

Archaeologists may consider reaching out to [open-archaeo.info](https://open-archaeo.info/), which is a list of open source software and resources developed by and for archaeologists. [AncientMetagenomeDir](https://www.spaam-community.org/AncientMetagenomeDir/#/) is a similar resource specializing on tools developed in support of ancient DNA research.

Additionally, the [archaeology CRAN task view](https://github.com/benmarwick/ctv-archaeology) lists numerous studies that include publicly accessible R code, which is a great resource for understanding how the community uses R in their research.

### Post about your work
Share the cool things you make, and communicate the fact that anyone can use or adapt them!
Even if the code is messy, it may inspire others to learn about your methods and extend upon what you've already done.

### Post contributor guidelines
Contributor guidelines inform prospective contributors about how they should engage with the codebase.
They may include requests to focus on specific aspects of the project, and requirements to provide specific information in pull requests or issues.
They may also outline the parameters through which code is vetted before being folded into the main branch.

Contributing guidelines are often specified in a [CONTRIBUTING.md file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/setting-guidelines-for-repository-contributors).
The Good Docs Project has written about the value of [contributing guidelines](https://www.thegooddocsproject.dev/template/contributing-guide), and has also provided [a great template](https://gitlab.com/tgdp/templates/-/blob/main/contributing-guide/template_contributing-guide.md) for you to adapt for your own project.

### Post a code of conduct
A code of conduct demonstrates that you are able to foster a warm and welcoming community.
They accomplish this by encouraging ethical and inclusive behaviour and by establishing parameters for a collaborative, respectful and safe work environment.

A code of conduct also needs to be enforceable.
It is therefore also necessary to include a response plan that outlines how incidents should be reported and documented, who will receive and handle these incidents, how incidents will be investigated and resolved, and how appeals and conflicts of interests will be handled.

Codes of conduct are often specified in a [CODE_OF_CONDUCT.md file](https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-code-of-conduct-to-your-project).
The Good Docs Project has written extensively about [codes of conduct](https://www.thegooddocsproject.dev/template/code-of-conduct), and has also provided templates documents for [codes of conduct](https://gitlab.com/tgdp/templates/-/blob/main/code-of-conduct/template_code-of-conduct.md), [response plans](https://gitlab.com/tgdp/templates/-/blob/main/code-of-conduct-response-plan/template_code-of-conduct-response-plan.md), and means of documenting and investigating incidents.

### Donate to FOSS projects
While many archaeologists who write code do benefit from a stable income, many do not. In fact, archaeologists who write code tend to be precariously employed. Even a small donation shows deep appreciation, and can help motivate labour performed largely by volunteers.

If you are able to provide financial support, please reach out to the maintainers of your most-used tools to help compensate them for their labour!

### Perform a code review
You may think, wait, is this something for a maintainathon? We say yes! Software reviews are valuable for people maintaining the software. It may give hints about what to improve and what to keep just as it is, as it works perfectly already. A review also raises awareness for the software, which is great for developers and potential users alike.

The [Journal of Open Source Software](https://joss.theoj.org/about) is one such place to write about research software. They have a certain scope and focus on the developer side of things. If there is a smaller software package you would wish to review or you would like to focus on less technical details, other journals may be a better place. For example, Archäologische Informationen are happy to take software reviews (such as [Thiery et al 2022](https://doi.org/10.11588/ai.2022.1.95272)).

This journal also published [Recommendations for the review of archaeological research software](https://doi.org/10.11588/ai.2020.1.81423), which might help you think of questions to ask yourself in regards to the software you are reviewing. Smaller reviews may also be a good topic for a blog or a social media post. Such appraisals are very valuable for researchers looking for the best solution to their problem!

## Technical
### Test your code's functionality
Testing your code involves including additional logic to test whether your code is operating as you expect it to. It is usually a good idea to write these unit tests before you write your code, and really think about what you want the outcome of each function or method to be. And then, as you write the function, test for that outcome as you iterate the development of that function. But often, in the excitement of working on a new project, we forget to test everything.

Writing tests helps you save your code from yourself: If you change things later on, you can make sure that you still get the same results. For more complex projects, you will also want to add integration tests that check whether all the parts work together in the way that you want them to.

Good tests help you make sure that your script, package, or software is maintainable and functional, and does exactly what you want it to. Also, tests are also great documentation! They tell others what you expected from your functions and methods, and help them get into working on your code.

Here are some useful resources for learning how tests can benefit you, and how to write them:

- Unit Tests and Integration Tests - [What's the difference?](https://www.geeksforgeeks.org/software-engineering/difference-between-unit-testing-and-integration-testing/)
- The package [testthat in R](https://testthat.r-lib.org/) and how to [use it in package development](https://r-pkgs.org/testing-basics.html).
- [Getting Started With Testing in Python](https://realpython.com/python-testing/)

Be sure to also check the resources about [Continuous Integration](https://md.archaeo.dev/zQnT2XfxSD-FTqsPzxF7Xw?both#Setup-CI) to see how you can automate your tests (and test your code on multiple operating systems)!

Adding more tests is a great thing to do in a maintainathon! We invite you to follow this grear example:

<details>
<summary>How to write unit tests</summary>

1. Take one function or method from a script, a package, a software that you produced.
2. Think long and hard about what you actually wanted this to do.
3. Produce some incoming test data, and some "ideal result" data.
4. Write at least one test that will make sure that one aspect of the result is really what you want it to be --- and share that using #DigiArchMaintainathon!

What do we mean by this? Let's say you developed a revolutionary new function that calculates the mean of some numbers, in R:

```r
revolutionary_new_mean <- function(x) {
  sum <- sum(x)
  n <- length(x)

  result <- sum / n

  return(result)
}
```

Think about what you want this result to be, and how your function could fail, and then see if it does or not:

```r
test_that("revolutionary_new_mean returns the correct mean for numeric vectors", {
  # Test with a basic vector of integers
  # Expected: (17 + 8 + 6 + 9) / 4 = 40 / 4 = 10
  result <- revolutionary_new_mean(c(17, 8, 6, 9))
  expect_equal(result, 10)
  
  # Test with decimal numbers
  # Expected: (17.5 + 8.5 + 6.5 + 9.5) / 4 = 42 / 4 = 10.5
  result <- my_mean(c(17.5, 8.5, 6.5, 9.5))
  expect_equal(result, 10.5)
})

test_that("revolutionary_new_mean handles non-numeric vectors", {
  # Expected: ???
  result <- revolutionary_new_mean(c("cake", "cheese"))
  # This is not valid R code.
  expect_true(result, ????)
})
```

Done! We now wrote two *unit tests* (even though one of them will fail miserably).

</details>

### [in progress:]{.mark} Optimizing for speed and efficiency

### [in progress:]{.mark} Setup Continuous Integration
Continuous integration (CI) refers to the process of automating tasks pertaining to the integration of changes and new features into existing software. The automatization process can be setup for different tasks such as building a software, testing, generating documentation, etc. There are several CI tools that can be used, one of the most accessible is Github Actions (but do check out GitLab CI if you work on Gitlab). 

Github Actions come in different flavors and can do many different things: for example, if you have setup a website using Github Pages, Github runs actions under the "hood" to build the website after each change committed. But Github Actions can be set-up for our own projects too. There is a whole [marketplace](https://github.com/marketplace?type=actions) of actions to choose from.

To set up an automatic Github action, one needs to go in the repository of the project and click on Actions, then click on "New Workflow".

![](images/github_actions_new-workflow.png)

Then, select the action you would like to set up. For example to set up automated testing on a Python software, you would choose "Python Application".

![](images/github_actions_application.png)

This will create a yaml script in the `.github/workflows` folder, that can be modified to correspond to one's own needs. For example, in the following file, the Github Action will perform three actions: 

```yaml
# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python application

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

permissions:
  contents: read

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.12
      uses: actions/setup-python@v3	# <1>
      with:
        python-version: "3.12"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip # <1>
        pip install flake8 pytest pytest-cov # <1> 
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi # <1>
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names 
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics # <2>
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics # <2>
    - name: Test with pytest and calculate coverage
      run: |
        pytest --cov-report "xml:coverage.xml" --cov=. # <3>
    - name: Create coverage
      if: ${{ github.event_name == 'pull_request' }}
      uses: orgoro/coverage@v3
      with: 
        coverageFile: coverage.xml
        token: ${{ secrets.GITHUB_TOKEN }}
```
1. setting up the Python version and installing the dependencies to run the Python application (necessitates a requirements.txt file in the directory), 
2. linting
3. testing using the functions defined by the user -- this last action also generates a test coverage report.

In another example, the following action is set up to deploy the documentation generated with Sphinx on the user's Github Pages instance.

```yaml
name: example

on: [push, pull_request, workflow_dispatch]

permissions:
  contents: write

jobs:
  docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
      - name: Install dependencies
        run: |
          pip install sphinx myst_parser
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Sphinx build
        run: |
          sphinx-apidoc -o ./docs/source ./metapypulation
          sphinx-build ./docs/source build/
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v4
        if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
        with:
          publish_branch: gh-pages
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: build/
          force_orphan: true
```

However you want to use CI, it is invaluable. After setting up an action for automated testing, for example, after each commit and push to the Github repository, you will get a notification of failed tests, or successful deployment, or whatever action you have set up for your project.

## Further Reading
### Research software engineering
Bast, R. et al. (2023) "How to document your research software - CodeRefinery lesson". Available at: https://zenodo.org/records/8280235 (Accessed: January 7, 2026).

Bast, R. et al. (2025) "Automated testing - CodeRefinery lesson". Available at: https://zenodo.org/records/16410888 (Accessed: January 7, 2026)

Coelho, J. and Valente, M.T. (2017) "Why modern open source projects fail," in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. ESEC/FSE’17: Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, Paderborn Germany: ACM, pp. 186–196. Available at: https://doi.org/10.1145/3106237.3106246.

Irving, D. et al. (2021) Research Software Engineering with Python. Available at: https://third-bit.com/py-rse/ (Accessed: January 2, 2026).

Nguyá»…n, S. and Rampin, V. (2022) "Who Writes Scholarly Code?," International Journal of Digital Curation, 17(1), p. 18. Available at: https://doi.org/10.2218/ijdc.v17i1.839.

Ratto, M. (2007) "A Practice-Based Model of Access for Science: Linux Kernel Development and Shared Digital Resources," Science & Technology Studies, 20(1), pp. 72–105. Available at: https://doi.org/10.23987/sts.55220.

Sholler, D. et al. (2019) "Ten Simple Rules for Helping Newcomers Become Contributors to Open Projects," p. 11. Available at: https://doi.org/10.1371/journal.pcbi.1007296.

Taschuk, M. and Wilson, G. (2017) "Ten simple rules for making research software more robust," PLOS Computational Biology, 13(4), p. e1005412. Available at: https://doi.org/10.1371/journal.pcbi.1005412.

Wilson, G. et al. (2014) "Best Practices for Scientific Computing," PLOS Biology, 12(1), p. e1001745. Available at: https://doi.org/10.1371/journal.pbio.1001745.

Wilson, G. et al. (2017) "Good enough practices in scientific computing," PLOS Computational Biology, 13(6), p. e1005510. Available at: https://doi.org/10.1371/journal.pcbi.1005510.

### Archaeological software development
Batist, Z. and Roe, J. (2024) "Open Archaeology, Open Source? Collaborative practices in an emerging community of archaeological software engineers," Internet Archaeology [Preprint], (67). Available at: https://doi.org/10.11141/ia.67.13.

Bogdani, J. and Sciacca, F. (2020) "An introspective, incomplete, view on the activity of the FLOS community dealing with Archaeology and Cultural Heritage," ARCHEO. FOSS XIV 2020, 15, p. 178.

Homburg, T. et al. (2021) "Recommendations for the review of archaeological research software," Archäologische Informationen, pp. 357-370 Seiten. Available at: https://doi.org/10.11588/AI.2020.1.81423.

Schmidt, S.C. and Marwick, B. (2020) "Tool-driven revolutions in archaeological science," Journal of Computer Applications in Archaeology, 3(1), pp. 18–32. Available at: https://doi.org/10.5334/jcaa.29.

Scollar, I. (1999) "25 Years of Computer Applications in Archaeology," in L. Dingwall et al. (eds.) Archaeology in the Age of the Internet. Computer Applications and Quantitative Methods in Archaeology, Oxford: Archaeopress, pp. 5–10. Available at: https://proceedings.caaconference.org/paper/02_scollar_caa_1997/.

Whallon, R. (1972) "The computer in archaeology: A critical survey," Computers and the Humanities, 7(1), pp. 29–45. Available at: https://doi.org/10.1007/BF02403759.

Wilson, A.T. and Edwards, B. (eds.) (2015) Open Source Archaeology: Ethics and Practice. De Gruyter Open Poland. Available at: https://doi.org/10.1515/9783110440171.

### Culture and history of FOSS
Coleman, E.G. (2012) Coding Freedom: The Ethics and Aesthetics of Hacking. Princeton University Press. Available at: https://doi.org/10.1515/9781400845293.

Kelty, C.M. (2008) Two bits: The cultural significance of free software. Duke University Press.

O’Neil, M. (2009) Cyberchiefs: Autonomy and authority in online tribes. London, UK: Pluto Press.
